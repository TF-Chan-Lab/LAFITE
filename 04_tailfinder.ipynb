{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: tailfinder.html\n",
    "title: tailFinder\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp tailFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from interlap import InterLap\n",
    "from joblib import Parallel, delayed\n",
    "from time import strftime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from LAFITE.utils import Vividict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "class AlternativeTerminalFinder:\n",
    "    def __init__(self, chrom, strand, corrected_read_splicing, read_info, min_count_tss_tes, max_sil=0):\n",
    "\n",
    "        self.chrom = chrom\n",
    "        self.strand = strand\n",
    "        self.corrected_read_splicing = corrected_read_splicing\n",
    "        self.polya_lst = read_info[2]\n",
    "        self.count = read_info[3]\n",
    "        self.fsm = read_info[4]\n",
    "        self.collapsed_ID = read_info[5]\n",
    "        # print(read_info,len(read_info))\n",
    "        self.reference_id = list(set(read_info[6]))\n",
    "        self.min_count_tss_tes = min_count_tss_tes\n",
    "        self.max_sil = max_sil\n",
    "        self.polya_count = sum(self.polya_lst)\n",
    "        if self.strand == '+':\n",
    "            self.rss_lst = read_info[0]\n",
    "            self.res_lst = read_info[1]\n",
    "        else:\n",
    "            self.rss_lst = read_info[1]\n",
    "            self.res_lst = read_info[0]\n",
    "\n",
    "    def optimal_k(self, end_list):\n",
    "        max_sil = self.max_sil\n",
    "        k_optimal = 1\n",
    "        df = pd.DataFrame(end_list, columns=['tts'])\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"error\")\n",
    "            for k in range(2, 5):\n",
    "                try:\n",
    "                    gmm = GaussianMixture(\n",
    "                        n_components=k, random_state=0).fit(df)\n",
    "                    labels = gmm.predict(df)\n",
    "                    curr_sil = silhouette_score(df, labels, metric='euclidean')\n",
    "                    if max_sil < curr_sil:\n",
    "                        max_sil = curr_sil\n",
    "                        k_optimal = k\n",
    "                except:\n",
    "                    k_optimal = k-1\n",
    "                    break\n",
    "        return df, k_optimal\n",
    "\n",
    "    def terminal_cluster(self):\n",
    "        outlist = [self.chrom, self.strand, self.corrected_read_splicing, self.fsm,\n",
    "                   self.count, self.polya_count, self.collapsed_ID, self.reference_id]\n",
    "        for idx, tail_lst in enumerate([self.rss_lst, self.res_lst]):\n",
    "\n",
    "            if idx == 0:\n",
    "                min_dis = 50\n",
    "                polya_lst = []\n",
    "            else:\n",
    "                min_dis = 24\n",
    "                polya_lst = self.polya_lst\n",
    "\n",
    "            if len(tail_lst) == 1:\n",
    "                if idx == 0:\n",
    "                    outlist.extend([tail_lst[0], [tail_lst[0]]])\n",
    "                else:\n",
    "                    polya_tag = True if polya_lst[0] else False\n",
    "                    outlist.extend([tail_lst[0], [tail_lst[0]], polya_tag])\n",
    "            else:\n",
    "                apa_dict = {}\n",
    "                polya_tag = False\n",
    "                df, k_optimal = self.optimal_k(tail_lst)\n",
    "                gmm = GaussianMixture(\n",
    "                    n_components=k_optimal, random_state=0).fit(df)\n",
    "                labels = gmm.predict(df)\n",
    "                df['labels'] = labels\n",
    "                clusters = {k: v for k, v in Counter(\n",
    "                    df['labels']).items() if v > 2}\n",
    "                clusters = dict(\n",
    "                    sorted(clusters.items(), key=lambda e: e[1], reverse=True))\n",
    "\n",
    "                if sum(polya_lst)/(self.count) >= 0.4:\n",
    "                    df['polya'] = polya_lst\n",
    "                    for key, value in clusters.items():\n",
    "                        if df[(df['labels'] == key) & (df['polya'] == True)].shape[0]/df[df['labels'] == key].shape[0] >= 0.2:\n",
    "                            if apa_dict:\n",
    "                                apa_count = df[df['labels'] == key]['tts'].value_counts().tolist()[\n",
    "                                    0]\n",
    "                                if apa_count >= self.min_count_tss_tes:\n",
    "                                    apa_site = df[df['labels'] ==\n",
    "                                                  key]['tts'].value_counts().index[0]\n",
    "                                    for tmp_site, tmp_count in apa_dict.copy().items():\n",
    "                                        if abs(tmp_site-apa_site) < min_dis and apa_count > tmp_count[0]:\n",
    "                                            apa_dict[apa_site] = [\n",
    "                                                apa_count, value]\n",
    "                                            apa_dict.pop(tmp_site)\n",
    "                                            break\n",
    "                                        elif abs(tmp_site-apa_site) < min_dis and apa_count == tmp_count[0] and value > tmp_count[1]:\n",
    "                                            apa_dict[apa_site] = [\n",
    "                                                apa_count, value]\n",
    "                                            apa_dict.pop(tmp_site)\n",
    "                                            break\n",
    "                                        elif abs(tmp_site-apa_site) < min_dis:\n",
    "                                            pass\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            apa_dict[apa_site] = [\n",
    "                                                apa_count, value]\n",
    "                            else:\n",
    "                                apa_count = df[df['labels'] == key]['tts'].value_counts().tolist()[\n",
    "                                    0]\n",
    "                                if apa_count >= self.min_count_tss_tes:\n",
    "                                    apa_site = df[df['labels'] ==\n",
    "                                                  key]['tts'].value_counts().index[0]\n",
    "                                    apa_dict[apa_site] = [apa_count, value]\n",
    "                        if apa_dict:\n",
    "                            polya_tag = True\n",
    "                else:\n",
    "                    for key, value in clusters.items():\n",
    "                        if apa_dict:\n",
    "                            apa_count = df[df['labels'] == key]['tts'].value_counts().tolist()[\n",
    "                                0]\n",
    "                            if apa_count >= self.min_count_tss_tes:\n",
    "                                apa_site = df[df['labels'] ==\n",
    "                                              key]['tts'].value_counts().index[0]\n",
    "                                for tmp_site, tmp_count in apa_dict.copy().items():\n",
    "                                    if abs(tmp_site-apa_site) < min_dis and apa_count > tmp_count[0]:\n",
    "                                        apa_dict[apa_site] = [apa_count, value]\n",
    "                                        apa_dict.pop(tmp_site)\n",
    "                                        break\n",
    "                                    elif abs(tmp_site-apa_site) < min_dis and apa_count == tmp_count[0] and value > tmp_count[1]:\n",
    "                                        apa_dict[apa_site] = [apa_count, value]\n",
    "                                        apa_dict.pop(tmp_site)\n",
    "                                        break\n",
    "                                    elif abs(tmp_site-apa_site) < min_dis:\n",
    "                                        pass\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        apa_dict[apa_site] = [apa_count, value]\n",
    "                        else:\n",
    "                            apa_count = df[df['labels'] == key]['tts'].value_counts().tolist()[\n",
    "                                0]\n",
    "                            if apa_count >= self.min_count_tss_tes:\n",
    "                                apa_site = df[df['labels'] ==\n",
    "                                              key]['tts'].value_counts().index[0]\n",
    "                                apa_dict[apa_site] = [apa_count, value]\n",
    "\n",
    "                if apa_dict:\n",
    "                    apa_site = list(apa_dict.keys())\n",
    "                    end = apa_site[0]\n",
    "                else:\n",
    "                    if sum(polya_lst)/(self.count) >= 0.4:\n",
    "                        if df[df['polya'] == True].shape[0]/df.shape[0] >= 0.2:\n",
    "                            polya_tag = True\n",
    "                        df = df[['tts', 'polya']].value_counts(\n",
    "                        ).reset_index(name='counts')\n",
    "                        df = df.pivot(\n",
    "                            index='tts', columns='polya', values='counts')\n",
    "                        df = df.reset_index(level=['tts'])\n",
    "                        df = df.fillna(0)\n",
    "                        if False not in df:\n",
    "                            df[False] = 0\n",
    "                        df['ratio'] = df[True]/(df[False]+df[True])\n",
    "                        if self.strand == '+':\n",
    "                            df = df.sort_values(['ratio', True, 'tts', False], ascending=[\n",
    "                                                False, False, False, True])\n",
    "                        else:\n",
    "                            df = df.sort_values(['ratio', True, 'tts', False], ascending=[\n",
    "                                                False, False, True, True])\n",
    "                    else:\n",
    "                        df = df['tts'].value_counts().reset_index()\n",
    "                        df.columns = ['tts', 'counts']\n",
    "                        if (self.strand == '+' and idx == 1) or (self.strand == '-' and idx == 0):\n",
    "                            df = df.sort_values(\n",
    "                                ['counts', 'tts'], ascending=[False, False])\n",
    "                        elif (self.strand == '+' and idx == 0) or (self.strand == '-' and idx == 1):\n",
    "                            df = df.sort_values(\n",
    "                                ['counts', 'tts'], ascending=[False, True])\n",
    "                    end = df['tts'].iloc[0]\n",
    "                    apa_site = [end]\n",
    "                if idx == 0:\n",
    "                    outlist.extend([end, apa_site])\n",
    "                else:\n",
    "                    outlist.extend([end, apa_site, polya_tag])\n",
    "        return outlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "class TailFinderWrapper:\n",
    "    def __init__(self, collected_multi_exon_read, min_count_tss_tes, thread):\n",
    "        self.collected_multi_exon_read = collected_multi_exon_read\n",
    "        self.min_count_tss_tes = min_count_tss_tes\n",
    "        self.thread = thread\n",
    "\n",
    "    def job_precompute(self):\n",
    "        precompute_list = []\n",
    "        for (chrom, strand), read_dict in self.collected_multi_exon_read.items():\n",
    "            for corrected_read_splicing, read_info in read_dict.items():\n",
    "                precompute_list.append(AlternativeTerminalFinder(\n",
    "                    chrom, strand, corrected_read_splicing, read_info, self.min_count_tss_tes))\n",
    "\n",
    "        return precompute_list\n",
    "\n",
    "    def run(self):\n",
    "        precompute_list = self.job_precompute()\n",
    "        with Parallel(n_jobs=self.thread) as parallel:\n",
    "            results_lst = parallel(delayed(lambda x: x.terminal_cluster())(job) for job in tqdm(\n",
    "                precompute_list, desc=f'{strftime(\"%Y-%m-%d %H:%M:%S\")}: Calculating pupative TSS and TES for collapsed read'))\n",
    "\n",
    "        return results_lst\n",
    "\n",
    "    def return_extremum(self, strand, entry):\n",
    "\n",
    "        if strand == '+':\n",
    "            extremum = max(entry)\n",
    "        else:\n",
    "            extremum = min(entry)\n",
    "        return extremum\n",
    "\n",
    "    def result_collection(self):\n",
    "        results_lst = self.run()\n",
    "        processed_collected_multi_exon_read = Vividict()\n",
    "        three_prime_exon = defaultdict(set)\n",
    "        for result in results_lst:\n",
    "            chrom, strand, corrected_read_splicing, fsm, total_count, polya_count, collapsed_ID, reference_id, start, as_site, end, apa_site, polya_tag = result\n",
    "            extremum = self.return_extremum(strand, apa_site)\n",
    "            if strand == '+':\n",
    "                three_prime_exon[(chrom, strand)].add(\n",
    "                    (corrected_read_splicing[-1], extremum))\n",
    "            else:\n",
    "                three_prime_exon[(chrom, strand)].add(\n",
    "                    (extremum, corrected_read_splicing[0]))\n",
    "            processed_collected_multi_exon_read[(chrom, strand)][corrected_read_splicing] = [\n",
    "                start, end, total_count, polya_count, fsm, polya_tag, as_site, apa_site, collapsed_ID, reference_id]\n",
    "\n",
    "        # sort by read exon number\n",
    "\n",
    "        for branch in processed_collected_multi_exon_read:\n",
    "            tmp_dict = {}\n",
    "            for k in sorted(processed_collected_multi_exon_read[branch], key=len, reverse=True):\n",
    "                tmp_dict[k] = processed_collected_multi_exon_read[branch][k]\n",
    "            processed_collected_multi_exon_read[branch] = tmp_dict\n",
    "\n",
    "        # convert three_prime_exon dict to interlap structure\n",
    "        for i in three_prime_exon:\n",
    "            t = list(three_prime_exon[i])\n",
    "            three_prime_exon[i] = InterLap()\n",
    "            three_prime_exon[i].update(t)\n",
    "\n",
    "        return processed_collected_multi_exon_read, three_prime_exon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
